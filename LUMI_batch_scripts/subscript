#!/bin/bash

#SBATCH --account=project_465001379
#SBATCH -J hipace
#SBATCH -o %x-%j.out
#SBATCH -t 06:00:00
#SBATCH --partition=standard-g
#SBATCH --nodes=100
#SBATCH --ntasks-per-node=8
#SBATCH --gpus-per-node=8

export MPICH_GPU_SUPPORT_ENABLED=1

# note (12-12-22)
# this environment setting is currently needed on LUMI to work-around a
# known issue with Libfabric
export FI_MR_CACHE_MAX_COUNT=0  # libfabric disable caching
# or, less invasive:
export FI_MR_CACHE_MONITOR=memhooks  # alternative cache monitor
# note (9-2-22, OLCFDEV-1079)
# this environment setting is needed to avoid that rocFFT writes a cache in
# the home directory, which does not scale.
export ROCFFT_RTC_CACHE_PATH=/dev/null
export OMP_NUM_THREADS=1

# needed for high nz runs.
# if too many mpi messages are send, the hardware counters can overflow, see
# https://docs.nersc.gov/performance/network/
# export FI_CXI_RX_MATCH_MODE=hybrid

# setting correct CPU binding
# (see https://docs.lumi-supercomputer.eu/runjobs/scheduled-jobs/lumig-job/)
cat << EOF > select_gpu
#!/bin/bash

export ROCR_VISIBLE_DEVICES=\$SLURM_LOCALID
exec \$*
EOF

chmod +x ./select_gpu

CPU_BIND="map_cpu:49,57,17,25,1,9,33,41"

#srun -n 1 --cpu-bind=${CPU_BIND} ./select_gpu $HOME/src/hipace/build/bin/hipace inputs/inputs_front_eta8_propagated
srun -n 800 --cpu-bind=${CPU_BIND} ./select_gpu $HOME/src/hipace/build/bin/hipace inputs/inputs_front_eta16_baseline_evolution
rm -rf ./select_gpu
